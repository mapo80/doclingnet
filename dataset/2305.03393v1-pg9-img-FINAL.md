Optimized Table Tokenization for Table Structure Recognition

[Page-header]

order to compute the 'LED score: Interence timing results for all experiments were obtained hrom the same machine O= a Single core wich AMD BPYC 7763 CPU @2.45 GHz.

5.1 Hyper Parameter Optimization

We have chosen the Yub LabNet data set to pertorm HPO , since it includes a highly diverse set of tables. Also we report LED scores separately IOr Simple and It is complex tables (tables with cell spans). Results are presented in Table evident that with O2SL, our muodel achieves the same TED score and slightly better mAP scores in comparison to HTML, However OZISL yields & 2 speed up in the inference runtime over HTML.

representation Table HPO OTSL and HTML pertormed the OII same transformer-based TableFormer [9] architecture; trained only o Pub labNet p22]. Ef: Fects of reducing the # of layers in encoder and decoder stages of the model show that smaller  models trained on OTSL perform better; especially in  recognizing complex table structures, and maintain & much higher mAP score than Uhe HITML counterpart;

| Column 1 |
| --- |
| \[EDs mAP \[nlerence enc-layers dec-layers Language simple complex all (0.75) tlme (secs) OTSL 0.965 J.934 0.955 0.88 2.73 HTML 0.969 0.92 0.955 0.85 7 5.39 OTSL 0.938 0.904 0.927 0.853 1.97 HTML 0.952 0.909 0.938 0.843 3.77 OTSL 0.923 0.897 0.915 0.859 1.91 HTML 0.945 0.9U1 0.931 0.834 3.81 OTSL 0.952 0.92 0.942 0.857 1.22 HTML 0.944 J.903 0.931 0.824 |

*Table 1.*

5.2 Quantitative Results

We picked the model parameter configuration that produced the best prediction dec=6, heads=8) with PubTabNet  alone; then independently quality (enc=6, trained and evaluated it on thrce publicly available data sets: Pub LabNet 395k samples); Ein ZabNet (118k samples) and Pub Zables-IM (about IM samples)  Performance Lesults are presented in Iable: [} It is clearly evident that the model

We picked the model parameter configuration that produced the best prediction dec=6, heads=8) with PubTabNet  alone; then independently quality (enc=6, trained and evaluated it on thrce publicly available data sets: Pub LabNet 395k samples); Ein ZabNet (118k samples) and Pub Zables-IM (about IM samples)  Performance Lesults are presented in Iable: [} It is clearly evident that the model trained on OZSL outperfors HZML across the board, kceping high "TEDs and mAP scores even on difiicult financial tables FinTabNet) that contain sparse and large tables.

samples); Ein ZabNet (118k samples) and Pub Zables-IM (about IM samples)  Performance Lesults are presented in Iable: [} It is clearly evident that the model trained on OZSL outperfors HZML across the board, kceping high "TEDs and mAP scores even on difiicult financial tables FinTabNet) that contain sparse and large tables.

and large tables. Additionally; the results show that OTSL has an  advantage over  HTML when applied on a bigger data set like Pub Zables-1M aud achieves siguificantly improved scores Einally; (ISL achieves faster interence due to tewer decoding a result of the reduced sequence representation. steps which is
